{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8f276e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\n",
    "import warnings\n",
    "import json\n",
    "import re\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- PREVENT TRUNCATION ---\n",
    "pd.set_option('display.max_rows', None) \n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6799beae",
   "metadata": {},
   "source": [
    "# SkillDev Class - Initialization & Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4669512e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkillDev:\n",
    "    def __init__(self, file_path, use_llama=True, model_name=\"meta-llama/Llama-3.2-1B-Instruct\"):\n",
    "        print(f\"--- üöÄ Loading Dataset: {file_path} ---\")\n",
    "        self.df = pd.read_csv(file_path)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.use_llama = use_llama\n",
    "        self.original_df = self.df.copy()\n",
    "        \n",
    "        # Store all available columns for flexible filtering\n",
    "        self.all_columns = self.df.columns.tolist()\n",
    "        print(f\"üìã Available columns: {self.all_columns}\")\n",
    "        \n",
    "        # Initialize LLAMA Model\n",
    "        if self.use_llama:\n",
    "            print(f\"--- ü§ñ Loading LLAMA Model: {model_name} ---\")\n",
    "            try:\n",
    "                self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "                self.llama_model = AutoModelForCausalLM.from_pretrained(\n",
    "                    model_name,\n",
    "                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "                    device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "                    low_cpu_mem_usage=True\n",
    "                )\n",
    "                self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "                print(f\"‚úÖ LLAMA Model loaded successfully on {self.device}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Could not load LLAMA model: {e}\")\n",
    "                print(\"Continuing without LLAMA integration...\")\n",
    "                self.use_llama = False\n",
    "        \n",
    "        self._prepare_data()\n",
    "\n",
    "    def _prepare_data(self):\n",
    "        \"\"\"Prepare and convert numeric columns\"\"\"\n",
    "        # Identify numeric columns\n",
    "        numeric_cols = self.df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        \n",
    "        # Convert any string columns that should be numeric\n",
    "        for col in self.all_columns:\n",
    "            try:\n",
    "                self.df[col] = pd.to_numeric(self.df[col], errors='coerce')\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Default features (numeric columns, excluding ID-like columns)\n",
    "        self.features = [col for col in self.df.select_dtypes(include=[np.number]).columns \n",
    "                        if col.upper() not in ['ID', 'INDEX', 'SEX', 'Q8']]\n",
    "        \n",
    "        if not self.features:\n",
    "            self.features = self.df.select_dtypes(include=[np.number]).columns.tolist()[:4]\n",
    "        \n",
    "        print(f\"üìä Features for clustering: {self.features}\")\n",
    "\n",
    "    def _extract_keywords_and_params(self, prompt):\n",
    "        \"\"\"Extract keywords, numbers, and intent from prompt\"\"\"\n",
    "        prompt_lower = prompt.lower()\n",
    "        \n",
    "        # Extract numbers\n",
    "        numbers = re.findall(r'\\b\\d+\\b', prompt)\n",
    "        quantity = int(numbers[0]) if numbers else None\n",
    "        \n",
    "        # Predefined keyword mappings for different domains\n",
    "        keyword_map = {\n",
    "            'women': ['women', 'lady', 'female', 'woman', 'wife', 'mother'],\n",
    "            'farmers': ['farmer', 'farming', 'agricultural', 'agriculture', 'tractor', 'crop', 'harvest'],\n",
    "            'students': ['student', 'school', 'education', 'college', 'university', 'study'],\n",
    "            'elderly': ['elder', 'elderly', 'old', 'senior', 'retired', 'pension'],\n",
    "            'youth': ['youth', 'young', 'teenager', 'teen', 'adolescent'],\n",
    "            'disabled': ['disable', 'disability', 'wheelchair', 'blind', 'deaf', 'impair'],\n",
    "            'poor': ['poor', 'poverty', 'needy', 'destitute', 'impoverish', 'low income'],\n",
    "            'children': ['child', 'kid', 'infant', 'toddler', 'children'],\n",
    "            'health': ['health', 'medical', 'medicine', 'doctor', 'hospital', 'sick', 'disease'],\n",
    "            'education': ['education', 'school', 'book', 'learn', 'scholarship'],\n",
    "        }\n",
    "        \n",
    "        detected_keywords = []\n",
    "        for category, keywords in keyword_map.items():\n",
    "            if any(kw in prompt_lower for kw in keywords):\n",
    "                detected_keywords.append(category)\n",
    "        \n",
    "        # Extract resource type (sewing, tractor, book, etc.)\n",
    "        resources = ['sewing machine', 'tractor', 'book', 'food', 'medicine', 'shelter', 'clothing', \n",
    "                     'equipment', 'tool', 'supply', 'device', 'machine']\n",
    "        resource_type = None\n",
    "        for resource in resources:\n",
    "            if resource in prompt_lower:\n",
    "                resource_type = resource\n",
    "                break\n",
    "        \n",
    "        return {\n",
    "            'keywords': detected_keywords,\n",
    "            'quantity': quantity,\n",
    "            'resource': resource_type,\n",
    "            'full_text': prompt\n",
    "        }\n",
    "\n",
    "    def _llama_understand_intent(self, prompt):\n",
    "        \"\"\"Use LLAMA to understand user intent and extract parameters\"\"\"\n",
    "        if not self.use_llama:\n",
    "            return self._rule_based_intent(prompt)\n",
    "        \n",
    "        system_prompt = \"\"\"You are an AI assistant for welfare distribution. Analyze the prompt and extract:\n",
    "1. Target Demographics (who should receive benefits)\n",
    "2. Resource Type (what is being distributed)\n",
    "3. Selection Criteria (age, gender, income level, etc.)\n",
    "4. Quantity (if mentioned)\n",
    "\n",
    "Respond in JSON format: {\"demographics\": \"...\", \"resource\": \"...\", \"criteria\": \"...\", \"quantity\": null or number}\"\"\"\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            inputs = self.tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True)\n",
    "            if self.device == \"cuda\":\n",
    "                inputs = inputs.to(self.device)\n",
    "            \n",
    "            outputs = self.llama_model.generate(\n",
    "                inputs,\n",
    "                max_new_tokens=200,\n",
    "                temperature=0.3,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "            \n",
    "            response = self.tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
    "            print(f\"ü§ñ LLAMA Analysis: {response}\")\n",
    "            return response\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è LLAMA analysis failed: {e}\")\n",
    "            return self._rule_based_intent(prompt)\n",
    "\n",
    "    def _rule_based_intent(self, prompt):\n",
    "        \"\"\"Enhanced rule-based intent and parameter detection\"\"\"\n",
    "        params = self._extract_keywords_and_params(prompt)\n",
    "        return json.dumps(params)\n",
    "\n",
    "    def _smart_filter_data(self, params, original_df):\n",
    "        \"\"\"Intelligently filter data based on extracted parameters\"\"\"\n",
    "        filtered_df = original_df.copy()\n",
    "        keywords = params.get('keywords', [])\n",
    "        \n",
    "        # Filter based on keywords\n",
    "        for keyword in keywords:\n",
    "            if keyword == 'women' and 'SEX' in filtered_df.columns:\n",
    "                filtered_df = filtered_df[filtered_df['SEX'] == 2]\n",
    "            elif keyword == 'elderly' and 'AGE' in filtered_df.columns:\n",
    "                filtered_df = filtered_df[filtered_df['AGE'] >= 60]\n",
    "            elif keyword == 'youth' and 'AGE' in filtered_df.columns:\n",
    "                filtered_df = filtered_df[(filtered_df['AGE'] >= 18) & (filtered_df['AGE'] <= 35)]\n",
    "            elif keyword == 'children' and 'AGE' in filtered_df.columns:\n",
    "                filtered_df = filtered_df[filtered_df['AGE'] < 18]\n",
    "            elif keyword == 'poor' and 'Q45_A_1' in filtered_df.columns:\n",
    "                median_income = filtered_df['Q45_A_1'].median()\n",
    "                filtered_df = filtered_df[filtered_df['Q45_A_1'] < median_income]\n",
    "        \n",
    "        # If no filters matched, return full dataset\n",
    "        if filtered_df.empty or len(filtered_df) < len(original_df) and len(keywords) > 0:\n",
    "            if len(filtered_df) > 0:\n",
    "                return filtered_df\n",
    "        \n",
    "        return filtered_df if len(filtered_df) > 0 else original_df\n",
    "\n",
    "    def _analyze_clusters_detailed(self, target_group, n_clusters=3):\n",
    "        \"\"\"Perform detailed cluster analysis\"\"\"\n",
    "        if target_group.empty:\n",
    "            print(\"‚ö†Ô∏è Empty dataset for clustering\")\n",
    "            return None, None\n",
    "        \n",
    "        # Get numeric features\n",
    "        numeric_features = [col for col in self.features if col in target_group.columns]\n",
    "        if not numeric_features:\n",
    "            numeric_features = target_group.select_dtypes(include=[np.number]).columns.tolist()[:4]\n",
    "        \n",
    "        # Fill missing values\n",
    "        for col in numeric_features:\n",
    "            target_group[col] = pd.to_numeric(target_group[col], errors='coerce').fillna(target_group[col].median())\n",
    "        \n",
    "        # Clustering\n",
    "        X = self.scaler.fit_transform(target_group[numeric_features])\n",
    "        kmeans = KMeans(n_clusters=min(n_clusters, len(target_group)), n_init=10, random_state=42)\n",
    "        target_group['cluster_id'] = kmeans.fit_predict(X)\n",
    "        \n",
    "        # Detailed cluster summary\n",
    "        summary = target_group.groupby('cluster_id')[numeric_features].agg(['mean', 'min', 'max', 'std'])\n",
    "        cluster_sizes = target_group.groupby('cluster_id').size()\n",
    "        \n",
    "        analysis = {\n",
    "            'summary': summary,\n",
    "            'sizes': cluster_sizes,\n",
    "            'data': target_group,\n",
    "            'features': numeric_features,\n",
    "            'kmeans': kmeans  # Store KMeans object for later use\n",
    "        }\n",
    "        \n",
    "        return analysis, numeric_features\n",
    "\n",
    "    def _find_nearest_cluster_members(self, analysis, top_k=20):\n",
    "        \"\"\"Find top members closest to cluster centers\"\"\"\n",
    "        target_group = analysis['data']\n",
    "        numeric_features = analysis['features']\n",
    "        kmeans = analysis['kmeans']\n",
    "        \n",
    "        # Find neediest cluster\n",
    "        income_col = 'Q45_A_1' if 'Q45_A_1' in numeric_features else numeric_features[0]\n",
    "        cluster_summary = analysis['summary'][income_col]['mean']\n",
    "        neediest_id = cluster_summary.idxmin()\n",
    "        \n",
    "        # Get data for neediest cluster\n",
    "        cluster_data = target_group[target_group['cluster_id'] == neediest_id]\n",
    "        X_cluster = self.scaler.transform(cluster_data[numeric_features])\n",
    "        \n",
    "        # Calculate distance to cluster center\n",
    "        center = kmeans.cluster_centers_[neediest_id]\n",
    "        distances = np.linalg.norm(X_cluster - center, axis=1)\n",
    "        \n",
    "        # Get top_k closest members\n",
    "        closest_indices = np.argsort(distances)[:top_k]\n",
    "        closest_members = cluster_data.iloc[closest_indices]\n",
    "        \n",
    "        return closest_members, neediest_id\n",
    "\n",
    "    def _llama_generate_intelligent_explanation(self, analysis, target_group, intent_params):\n",
    "        \"\"\"Generate contextual explanation using LLAMA\"\"\"\n",
    "        if not self.use_llama or analysis is None:\n",
    "            return self._generate_basic_explanation(analysis, target_group, intent_params)\n",
    "        \n",
    "        # Find neediest cluster (lowest average income if available)\n",
    "        income_col = 'Q45_A_1' if 'Q45_A_1' in analysis['features'] else analysis['features'][0]\n",
    "        cluster_summary = analysis['summary'][income_col]['mean']\n",
    "        \n",
    "        if cluster_summary.empty:\n",
    "            return self._generate_basic_explanation(analysis, target_group, intent_params)\n",
    "        \n",
    "        neediest_id = cluster_summary.idxmin()\n",
    "        eligible_count = len(target_group[target_group['cluster_id'] == neediest_id])\n",
    "        \n",
    "        summary_text = str(analysis['summary'].round(2))\n",
    "        \n",
    "        prompt = f\"\"\"Analyze this welfare distribution scenario:\n",
    "\n",
    "Dataset Size: {len(target_group)} people\n",
    "Number of Clusters: {len(analysis['sizes'])}\n",
    "Cluster Distribution: {analysis['sizes'].to_dict()}\n",
    "\n",
    "Intent/Keywords: {intent_params.get('keywords', [])}\n",
    "Resource Type: {intent_params.get('resource', 'General aid')}\n",
    "Quantity Needed: {intent_params.get('quantity', 'Not specified')}\n",
    "\n",
    "Cluster Statistics:\n",
    "{summary_text}\n",
    "\n",
    "Selected Target: Cluster {neediest_id} with {eligible_count} eligible recipients\n",
    "\n",
    "Provide a 2-3 sentence explanation of:\n",
    "1. Why this cluster was selected\n",
    "2. What this distribution means for the beneficiaries\n",
    "3. Expected impact\"\"\"\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a compassionate welfare officer explaining distribution decisions.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            inputs = self.tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True)\n",
    "            if self.device == \"cuda\":\n",
    "                inputs = inputs.to(self.device)\n",
    "            \n",
    "            outputs = self.llama_model.generate(\n",
    "                inputs,\n",
    "                max_new_tokens=250,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "            \n",
    "            explanation = self.tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
    "            return explanation.strip()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Explanation generation failed: {e}\")\n",
    "            return self._generate_basic_explanation(analysis, target_group, intent_params)\n",
    "\n",
    "    def _generate_basic_explanation(self, analysis, target_group, intent_params):\n",
    "        \"\"\"Generate basic explanation without LLAMA\"\"\"\n",
    "        if analysis is None:\n",
    "            return \"Unable to analyze clusters with given filters.\"\n",
    "        \n",
    "        income_col = 'Q45_A_1' if 'Q45_A_1' in analysis['features'] else analysis['features'][0]\n",
    "        cluster_summary = analysis['summary'][income_col]['mean']\n",
    "        neediest_id = cluster_summary.idxmin()\n",
    "        eligible_count = len(target_group[target_group['cluster_id'] == neediest_id])\n",
    "        \n",
    "        explanation = f\"\"\"Distribution Analysis Results:\n",
    "- Selected Cluster: {neediest_id} (Most in need)\n",
    "- Eligible Recipients: {eligible_count}\n",
    "- Target Demographics: {', '.join(intent_params.get('keywords', ['General population']))}\n",
    "- Resource: {intent_params.get('resource', 'General aid')}\n",
    "- Quantity Available: {intent_params.get('quantity', 'Not specified')}\"\"\"\n",
    "        \n",
    "        return explanation\n",
    "\n",
    "    def run_scenario(self, prompt, n_clusters=3):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"üí¨ USER PROMPT: \\\"{prompt}\\\"\")\n",
    "        print('='*80)\n",
    "        \n",
    "        # 1. Parse intent and extract parameters\n",
    "        intent_result = self._llama_understand_intent(prompt)\n",
    "        try:\n",
    "            intent_params = json.loads(intent_result) if intent_result.startswith('{') else self._extract_keywords_and_params(prompt)\n",
    "        except:\n",
    "            intent_params = self._extract_keywords_and_params(prompt)\n",
    "        \n",
    "        print(f\"\\nüìã EXTRACTED PARAMETERS:\")\n",
    "        print(f\"   Keywords: {intent_params.get('keywords', [])}\")\n",
    "        print(f\"   Resource: {intent_params.get('resource', 'Not specified')}\")\n",
    "        print(f\"   Quantity: {intent_params.get('quantity', 'Not specified')}\")\n",
    "\n",
    "        # 2. Smart filtering based on parameters\n",
    "        target_group = self._smart_filter_data(intent_params, self.original_df)\n",
    "        \n",
    "        print(f\"\\nüë• FILTERED POPULATION: {len(target_group)} people (from {len(self.original_df)} total)\")\n",
    "        \n",
    "        if target_group.empty:\n",
    "            print(\"‚ö†Ô∏è No matching records found with current filters.\")\n",
    "            return\n",
    "\n",
    "        # 3. Cluster Analysis\n",
    "        print(f\"\\nüìä K-MEANS CLUSTERING (n_clusters={n_clusters})\")\n",
    "        analysis, features = self._analyze_clusters_detailed(target_group, n_clusters)\n",
    "        \n",
    "        if analysis is None:\n",
    "            print(\"‚ö†Ô∏è Could not perform cluster analysis\")\n",
    "            return\n",
    "        \n",
    "        # 4. Display Cluster Profiles\n",
    "        print(f\"\\n--- üìä CLUSTER PROFILES ---\")\n",
    "        print(f\"Total Clusters: {len(analysis['sizes'])}\")\n",
    "        print(f\"Cluster Sizes: {analysis['sizes'].to_dict()}\")\n",
    "        print(f\"\\nDetailed Statistics:\")\n",
    "        print(analysis['summary'])\n",
    "\n",
    "        # 5. Generate intelligent explanation\n",
    "        explanation = self._llama_generate_intelligent_explanation(analysis, target_group, intent_params)\n",
    "        print(f\"\\nüìå AI DECISION & EXPLANATION:\")\n",
    "        print(f\"{explanation}\")\n",
    "        \n",
    "        # 6. Show eligible recipients (closest to cluster center)\n",
    "        print(f\"\\nüì¢ ELIGIBLE RECIPIENTS (Closest to Cluster Center)\")\n",
    "        closest_members, neediest_id = self._find_nearest_cluster_members(analysis, top_k=20)\n",
    "        \n",
    "        print(f\"Cluster {neediest_id}: {len(closest_members)} top candidates\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        display_cols = [col for col in ['AGE', 'SEX', 'Q8', 'Q20', 'Q45_A_1'] if col in closest_members.columns]\n",
    "        if not display_cols:\n",
    "            display_cols = closest_members.select_dtypes(include=[np.number]).columns.tolist()[:5]\n",
    "        \n",
    "        print(closest_members[display_cols].head(20))\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        return {\n",
    "            'target_group': target_group,\n",
    "            'closest_members': closest_members,\n",
    "            'analysis': analysis,\n",
    "            'intent_params': intent_params\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfeb2f92",
   "metadata": {},
   "source": [
    "## NLP & Intent Extraction Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b35a493",
   "metadata": {},
   "source": [
    "## System Initialization & Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb645c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"üîß Initializing SkillDev System with Advanced Analytics...\\n\")\n",
    "\n",
    "# Check for CSV file - look in data folder\n",
    "csv_file = 'data/LFS-2023.csv'\n",
    "if not os.path.exists(csv_file):\n",
    "    # Fallback to current directory\n",
    "    csv_file = 'LFS-2023.csv'\n",
    "    \n",
    "if not os.path.exists(csv_file):\n",
    "    print(f\"‚ö†Ô∏è File 'LFS-2023.csv' not found!\")\n",
    "    print(f\"üìÇ Current directory: {os.getcwd()}\\n\")\n",
    "    print(\"üí° Options:\")\n",
    "    print(\"   1. Place 'LFS-2023.csv' in the data folder\")\n",
    "    print(\"   2. Use a different file path\")\n",
    "    print(\"   3. Generate sample data for testing\\n\")\n",
    "    \n",
    "    # Option to generate sample data\n",
    "    response = input(\"Generate sample data for testing? (yes/no): \").strip().lower()\n",
    "    if response == 'yes' or response == 'y':\n",
    "        print(\"\\nüî® Generating sample dataset...\")\n",
    "        import numpy as np\n",
    "        \n",
    "        # Generate realistic sample data\n",
    "        np.random.seed(42)\n",
    "        n_samples = 1000\n",
    "        \n",
    "        sample_data = pd.DataFrame({\n",
    "            'AGE': np.random.randint(18, 80, n_samples),\n",
    "            'SEX': np.random.choice([1, 2], n_samples),  # 1=Male, 2=Female\n",
    "            'Q8': np.random.randint(1, 10, n_samples),  # Education/occupation code\n",
    "            'Q20': np.random.randint(0, 60, n_samples),  # Weekly hours worked\n",
    "            'Q45_A_1': np.random.exponential(50000, n_samples),  # Income (exponential distribution)\n",
    "            'EDU': np.random.randint(1, 15, n_samples)  # Years of education\n",
    "        })\n",
    "        \n",
    "        sample_data.to_csv(csv_file, index=False)\n",
    "        print(f\"‚úÖ Sample dataset created: {csv_file} ({n_samples} records)\")\n",
    "        print(f\"   Age: 18-80, Sex: Male/Female, Income: varied distribution\\n\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Please place 'LFS-2023.csv' in the data folder\")\n",
    "\n",
    "# Initialize with LLAMA integration\n",
    "# Note: You may need to login to HuggingFace and accept LLAMA model terms\n",
    "# Run: huggingface-cli login\n",
    "system = SkillDev(csv_file, use_llama=False)  # Set to False to skip LLAMA for faster testing\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üöÄ SYSTEM READY - Enter prompts to analyze and distribute resources\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "while True:\n",
    "    prompt = input(\"\\nüìù Enter a prompt (or 'quit' to exit): \")\n",
    "    if prompt.lower() == 'quit':\n",
    "        break\n",
    "    if prompt.strip():\n",
    "        system.run_scenario(prompt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9d0345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Run without LLAMA integration or test specific scenarios\n",
    "system_basic = SkillDev('LFS-2023.csv', use_llama=False)\n",
    "\n",
    "# Test scenarios - the model now handles ANY type of prompt\n",
    "print(\"\\nüß™ TESTING MULTIPLE SCENARIOS:\\n\")\n",
    "\n",
    "test_prompts = [\n",
    "    \"I have 100 sewing machines to distribute to vulnerable women\",\n",
    "    \"Find the poorest people in the dataset who need medical support\",\n",
    "    \"Identify young people aged 18-35 for skill development programs\",\n",
    "    \"We have tractors available for farmers in the agriculture sector\",\n",
    "    \"Select elderly people over 60 who need financial assistance\",\n",
    "    \"Find children and students who need educational materials\"\n",
    "]\n",
    "\n",
    "for test_prompt in test_prompts:\n",
    "    try:\n",
    "        result = system_basic.run_scenario(test_prompt)\n",
    "        print(\"\\n‚úÖ Scenario completed successfully\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error in scenario: {e}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
