{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8f276e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üöÄ Loading Dataset: LFS-2023.csv ---\n",
      "--- ü§ñ Loading LLAMA Model: meta-llama/Llama-3.2-1B-Instruct ---\n",
      "‚ö†Ô∏è Could not load LLAMA model: You are trying to access a gated repo.\n",
      "Make sure to have access to it at https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct.\n",
      "401 Client Error. (Request ID: Root=1-69800476-682281972aa43f7c1cc35c7d;1e2f944b-5fea-4f65-96bc-e633ae8639d3)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.\n",
      "Continuing without LLAMA integration...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- PREVENT TRUNCATION ---\n",
    "pd.set_option('display.max_rows', None) \n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "class SkillDev:\n",
    "    def __init__(self, file_path, use_llama=True, model_name=\"meta-llama/Llama-3.2-1B-Instruct\"):\n",
    "        print(f\"--- üöÄ Loading Dataset: {file_path} ---\")\n",
    "        self.df = pd.read_csv(file_path)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.use_llama = use_llama\n",
    "        \n",
    "        # Initialize LLAMA4 Model\n",
    "        if self.use_llama:\n",
    "            print(f\"--- ü§ñ Loading LLAMA Model: {model_name} ---\")\n",
    "            try:\n",
    "                self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "                self.llama_model = AutoModelForCausalLM.from_pretrained(\n",
    "                    model_name,\n",
    "                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "                    device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "                    low_cpu_mem_usage=True\n",
    "                )\n",
    "                self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "                print(f\"‚úÖ LLAMA Model loaded successfully on {self.device}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Could not load LLAMA model: {e}\")\n",
    "                print(\"Continuing without LLAMA integration...\")\n",
    "                self.use_llama = False\n",
    "        \n",
    "        self._prepare_data()\n",
    "\n",
    "    def _prepare_data(self):\n",
    "        cols = ['AGE', 'EDU', 'Q8', 'Q20', 'Q45_A_1', 'SEX']\n",
    "        for col in cols:\n",
    "            if col in self.df.columns:\n",
    "                self.df[col] = pd.to_numeric(self.df[col], errors='coerce').fillna(0)\n",
    "        self.features = ['AGE', 'EDU', 'Q20', 'Q45_A_1']\n",
    "\n",
    "    def _llama_understand_intent(self, prompt):\n",
    "        \"\"\"Use LLAMA to understand user intent and extract parameters\"\"\"\n",
    "        if not self.use_llama:\n",
    "            return self._rule_based_intent(prompt)\n",
    "        \n",
    "        system_prompt = \"\"\"You are an AI assistant for a welfare distribution system. \n",
    "Analyze the user's request and determine:\n",
    "1. Target Group: 'women_sewing', 'farmers', or 'general'\n",
    "2. Resource Type: what is being distributed\n",
    "3. Number of items: if mentioned\n",
    "\n",
    "Respond in JSON format: {\"target_group\": \"...\", \"resource\": \"...\", \"quantity\": number or null}\"\"\"\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            inputs = self.tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True)\n",
    "            if self.device == \"cuda\":\n",
    "                inputs = inputs.to(self.device)\n",
    "            \n",
    "            outputs = self.llama_model.generate(\n",
    "                inputs,\n",
    "                max_new_tokens=150,\n",
    "                temperature=0.3,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "            \n",
    "            response = self.tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
    "            print(f\"ü§ñ LLAMA Intent Analysis: {response}\")\n",
    "            \n",
    "            # Parse LLAMA response (simple fallback if JSON parsing fails)\n",
    "            if \"women_sewing\" in response.lower() or \"sewing\" in prompt.lower():\n",
    "                return \"women_sewing\"\n",
    "            elif \"farmer\" in response.lower() or \"farmer\" in prompt.lower() or \"tractor\" in prompt.lower():\n",
    "                return \"farmers\"\n",
    "            else:\n",
    "                return \"general\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è LLAMA intent analysis failed: {e}\")\n",
    "            return self._rule_based_intent(prompt)\n",
    "\n",
    "    def _rule_based_intent(self, prompt):\n",
    "        \"\"\"Fallback rule-based intent detection\"\"\"\n",
    "        if \"sewing\" in prompt.lower():\n",
    "            return \"women_sewing\"\n",
    "        elif \"farmer\" in prompt.lower() or \"tractor\" in prompt.lower():\n",
    "            return \"farmers\"\n",
    "        else:\n",
    "            return \"general\"\n",
    "\n",
    "    def _llama_generate_explanation(self, cluster_summary, target_cluster, eligible_count, intent):\n",
    "        \"\"\"Use LLAMA to generate natural language explanation of AI decision\"\"\"\n",
    "        if not self.use_llama:\n",
    "            return f\"Selected Cluster {target_cluster} with {eligible_count} recipients (Lowest Income Group).\"\n",
    "        \n",
    "        summary_text = cluster_summary.to_string()\n",
    "        \n",
    "        prompt = f\"\"\"Based on this welfare distribution analysis:\n",
    "\n",
    "Cluster Statistics:\n",
    "{summary_text}\n",
    "\n",
    "Selected Cluster: {target_cluster}\n",
    "Number of Eligible Recipients: {eligible_count}\n",
    "Distribution Type: {intent}\n",
    "\n",
    "Provide a clear, empathetic explanation (2-3 sentences) of why this cluster was selected and what it means for the beneficiaries.\"\"\"\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a compassionate AI welfare officer explaining distribution decisions.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            inputs = self.tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True)\n",
    "            if self.device == \"cuda\":\n",
    "                inputs = inputs.to(self.device)\n",
    "            \n",
    "            outputs = self.llama_model.generate(\n",
    "                inputs,\n",
    "                max_new_tokens=200,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "            \n",
    "            explanation = self.tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
    "            return explanation.strip()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è LLAMA explanation generation failed: {e}\")\n",
    "            return f\"Selected Cluster {target_cluster} with {eligible_count} recipients (Lowest Income Group).\"\n",
    "\n",
    "    def run_scenario(self, prompt, n_clusters=3):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"üí¨ PROMPT: \\\"{prompt}\\\"\")\n",
    "        \n",
    "        # 1. LLAMA-Enhanced Intent Understanding\n",
    "        intent = self._llama_understand_intent(prompt)\n",
    "        \n",
    "        if intent == \"women_sewing\":\n",
    "            target_group = self.df[(self.df['SEX'] == 2) & (self.df['Q8'].astype(str).str.startswith(('7', '9')))].copy()\n",
    "            label = \"Vulnerable Women for Sewing Machines\"\n",
    "        elif intent == \"farmers\":\n",
    "            target_group = self.df[self.df['Q8'].astype(str).str.startswith('6')].copy()\n",
    "            label = \"Agricultural Sector for Tractors/Fertilizer\"\n",
    "        else:\n",
    "            target_group = self.df.copy()\n",
    "            label = \"General Population\"\n",
    "\n",
    "        if target_group.empty:\n",
    "            print(\"‚ö†Ô∏è No matching records found.\")\n",
    "            return\n",
    "\n",
    "        # 2. AI Clustering\n",
    "        X = self.scaler.fit_transform(target_group[self.features])\n",
    "        kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
    "        target_group['cluster_id'] = kmeans.fit_predict(X)\n",
    "\n",
    "        # 3. Cluster Summary\n",
    "        summary = target_group.groupby('cluster_id')[self.features].mean()\n",
    "        summary['Size'] = target_group.groupby('cluster_id').size()\n",
    "        print(f\"\\n--- üìä CLUSTER PROFILES ({label}) ---\")\n",
    "        print(summary.rename(columns={'Q45_A_1': 'Avg Income (LKR)', 'Q20': 'Weekly Hours'}).round(2))\n",
    "\n",
    "        # 4. Identify Target Cluster\n",
    "        neediest_id = summary['Q45_A_1'].idxmin()\n",
    "        all_eligible = target_group[target_group['cluster_id'] == neediest_id].copy()\n",
    "\n",
    "        # 5. LLAMA-Generated Explanation\n",
    "        explanation = self._llama_generate_explanation(summary, neediest_id, len(all_eligible), intent)\n",
    "        print(f\"\\nüìå AI DECISION (LLAMA-Enhanced):\")\n",
    "        print(f\"{explanation}\")\n",
    "        print(f\"\\nüì¢ FULL ELIGIBILITY LIST: Found {len(all_eligible)} recipients.\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        print(all_eligible[['AGE', 'SEX', 'Q8', 'Q20', 'Q45_A_1']])\n",
    "        print(\"-\" * 70)\n",
    "\n",
    "# --- EXECUTION ---\n",
    "# Initialize with LLAMA4 integration\n",
    "# Note: You may need to login to HuggingFace and accept LLAMA model terms\n",
    "# Run: huggingface-cli login\n",
    "system = SkillDev('LFS-2023.csv', use_llama=True)\n",
    "\n",
    "\n",
    "while True:\n",
    "    n=input(\"Enter prompts\")\n",
    "    system.run_scenario(n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313d3902",
   "metadata": {},
   "source": [
    "## Alternative: Run Without LLAMA (if model loading fails)\n",
    "\n",
    "If you encounter issues loading LLAMA or want to run without it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9d0345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run without LLAMA integration\n",
    "system_basic = SkillDev('LFS-2023.csv', use_llama=False)\n",
    "system_basic.run_scenario(\"I have 100 sewing machines find suitable people for that\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
